readr: read_csv & read_tsv

install.packages("readr")
library(readr)

#classic read from util package
read.csv("data.csv", stringsAsFactors = FALSE)

#readr version is faster
read_csv("data.csv")

##TSV files
#classic util version
read.delim("data.csv", stringsAsFactors = FALSE)

#readr version
read_tsv("data.csv")

#we can specify column names:
# Column names
properties <- c("area", "temp", "size", "storage", "method", "texture", "flavor", "moistness")

# Import potatoes.txt: 
potatoes <- read_tsv("potatoes.txt", col_names = properties)

# Call head() on potatoes
head(potatoes)
# A tibble: 6 x 8
   area  temp  size storage method texture flavor moistness
  <int> <int> <int>   <int>  <int>   <dbl>  <dbl>     <dbl>
1     1     1     1       1      1     2.9    3.2       3  
2     1     1     1       1      2     2.3    2.5       2.6
3     1     1     1       1      3     2.5    2.8       2.8
4     1     1     1       1      4     2.1    2.9       2.4
5     1     1     1       1      5     1.9    2.8       2.2
6     1     1     1       2      1     1.8    3         1.7


##reading a txt datafile with an unusual specifier
#standard util version
read.table("data.txt", header = TRUE, sep = "/", stringsAsFactors = FALSE)

#readr version with specified separator
read_delim("data.txt", delim = "/")

#if the file uses tabs ("\t") to delimit values and does not contain column names in its first line:
potatoes <- read_delim("potatoes.txt", delim = "\t", col_names = properties)

#col_names argument is used to specify column names
read_delim("data.txt", delim = "/", col_names = FALSE) #gets rid of default column names

read_delim("data.txt", delim = "/", col_names = c("state", "city", "pop", "area"))


##column types: column classes (col_types) is guessed based on the first 30 rows of the input
# we can specify col_types as c for character, d for double, i for integer, l for logical, or _ to skip a column
read_delim("data.txt", delim = "/", col_types = "ccdd")


##working with BIGDATA (millions of rows)
# skip and n_max
read_delim("data.txt", delim = "/", skip = 2, n_max = 3)  #this command skips 2 rows and reads the next 3 records, but column names must be specified in this case
read_delim("data.txt", delim = "/", col_names = c("state", "city", "pop", "area"), skip = 2, n_max = 3)

#Exercise: skip rows of the data and import observations 7, 8, 9, 10 and 11 from potatoes.txt.
# Column names
properties <- c("area", "temp", "size", "storage", "method",
                "texture", "flavor", "moistness")

# Import 5 observations from potatoes.txt: potatoes_fragment
potatoes_fragment <- read_tsv("potatoes.txt", skip = 6, n_max = 5, col_names = properties)


##col_types with collectors
#Another way of setting the types of the imported columns is using collectors. 
#Collector functions can be passed in a list() to the col_types argument of read_ functions to tell them how to interpret 
#values in a column.

#For a complete list of collector functions, you can take a look at the collector documentation. 
#For this exercise you will need two collector functions:

col_integer(): the column should be interpreted as an integer.
col_factor(levels, ordered = FALSE): the column should be interpreted as a factor with levels.
In this exercise, you will work with hotdogs.txt, which is a tab-delimited file without column names in the first row.

#Exercise:hotdogs is created for you without setting the column types. Inspect its summary using the summary() function.
Two collector functions are defined for you: fac and int. Have a look at them, do you understand what they're collecting?

In the second read_tsv() call, edit the col_types argument: Pass a list() with the elements fac, int and int, 
so the first column is imported as a factor, and the second and third column as integers.
Create a summary() of hotdogs_factor. Compare this to the summary of hotdogs.

# readr is already loaded

# Import without col_types
hotdogs <- read_tsv("hotdogs.txt", col_names = c("type", "calories", "sodium"))

# Display the summary of hotdogs
summary(hotdogs)
    type              calories         sodium     
 Length:54          Min.   : 86.0   Min.   :144.0  
 Class :character   1st Qu.:132.0   1st Qu.:362.5  
 Mode  :character   Median :145.0   Median :405.0  
                    Mean   :145.4   Mean   :424.8  
                    3rd Qu.:172.8   3rd Qu.:503.5  
                    Max.   :195.0   Max.   :645.0
                    
# The collectors you will need to import the data
fac <- col_factor(levels = c("Beef", "Meat", "Poultry"))
int <- col_integer()

# Edit the col_types argument to import the data correctly: hotdogs_factor
hotdogs_factor <- read_tsv("hotdogs.txt",
                           col_names = c("type", "calories", "sodium"),
                           col_types = list(fac, int, int))

# Display the summary of hotdogs_factor
summary(hotdogs_factor)
     type       calories         sodium     
 Beef   :20   Min.   : 86.0   Min.   :144.0  
 Meat   :17   1st Qu.:132.0   1st Qu.:362.5  
 Poultry:17   Median :145.0   Median :405.0  
              Mean   :145.4   Mean   :424.8  
              3rd Qu.:172.8   3rd Qu.:503.5  
              Max.   :195.0   Max.   :645.0


##data.table
##data.table package is known for speed
data.table:fread
install.packages("data.table")
library(data.table)

#fread function in data.table package is powerful, it deals with column names automatically
fread("data.txt")

#fread infers column types and separators automatically

#fread can drop or select specific variables
#Suppose you have a dataset that contains 5 variables and you want to keep the first and fifth variable, named "a" and "e". 
#The following options will all do the trick:

fread("path/to/file.txt", drop = 2:4)
fread("path/to/file.txt", select = c(1, 5))
fread("path/to/file.txt", drop = c("b", "c", "d"))
fread("path/to/file.txt", select = c("a", "e"))


## EXCEL FILES

#readxl package
#readxl is very fast but still in development
library(readxl)
#excel_sheets() lists different sheets
#read_excel() imports excel data into R

#check files in the directory:
dir()
"cities.xlsx"

excel_sheets("cities.xlsx")
"year_1990" "year_2000"

read_excel("cities.xlsx") #imports first sheet in excel file by default
#we can read the second sheet by the following way:
read_excel("cities.xlsx", sheet = 2)
read_excel("cities.xlsx", sheet = "year_2000")


##reading a workbook
#Have a look at the example code below:

my_workbook <- lapply(excel_sheets("data.xlsx"),
                      read_excel,
                      path = "data.xlsx")

#The read_excel() function is called multiple times on the "data.xlsx" file and each sheet is loaded in one after the other. 
#The result is a list of data frames, each data frame representing one of the sheets in data.xlsx.


#other read_excel() arguments
read_excel(path, sheet =1,
           col_names = TRUE, col_types = NULL, skip = 0)

#col_names set to TRUE means first row is column names
#col_types NULL leaves R to guess the data types of the different columns
#skip argument specifies number of rows to be skipped
#n_max argument of readr not yet available in readxl

#if we want to import columns as text (or "numeric", "date", "blank")  for example:
read_excel("cities.xlsx", col_types = c("text", "text"))
#we can ignore a column by setting its type to blank:
read_excel("cities.xlsx", col_types = c("text", "blank")) #ignores second column in the excel sheet


##gdata package
read.xls() functionality reads xls files, it also supports xlsx with additional driver:
gdata: read.xls support for 'XLS' (Excel 97-2004) files ENABLED.
gdata: read.xls support for 'XLSX' (Excel 2007+) files ENABLED.
it uses Perl to import data from Excel
read.xls() is like an extention of read.table() function in R, so uses same arguments

read.xls("cities.xls", sheet = 2)

Exercise:
# Column names for urban_pop
columns <- c("country", paste0("year_", 1967:1974))

# Finish the read.xls call
urban_pop <- read.xls("urbanpop.xls", sheet = 2,
                      skip = 50, header = FALSE, stringsAsFactors = FALSE,
                      col.names = columns)

# Print first 10 observation of urban_pop
head(urban_pop, 10)
              country   year_1967   year_1968   year_1969   year_1970
1              Cyprus   231929.74   237831.38   243983.34   250164.52
2      Czech Republic  6204409.91  6266304.50  6326368.97  6348794.89
3             Denmark  3777552.62  3826785.08  3874313.99  3930042.97
4            Djibouti    77788.04    84694.35    92045.77    99845.22
5            Dominica    27550.36    29527.32    31475.62    33328.25
6  Dominican Republic  1535485.43  1625455.76  1718315.40  1814060.00
7             Ecuador  2059355.12  2151395.14  2246890.79  2345864.41
8               Egypt 13798171.00 14248342.19 14703858.22 15162858.52
9         El Salvador  1345528.98  1387218.33  1429378.98  1472181.26
10  Equatorial Guinea    75364.50    77295.03    78445.74    78411.07


#Exercise:
Work that Excel data!
Now that you can read in Excel data, let's try to clean and merge it. You already used the cbind() function some exercises ago. 
Let's take it one step further now.
The urbanpop.xls dataset is available in your working directory. The file still contains three sheets, and has column names 
in the first row of each sheet.

Extend the cbind() call so that it also includes urban_sheet3. Make sure the first column of urban_sheet2 and urban_sheet3 are removed, 
so you don't have duplicate columns. Store the result in urban.
Use na.omit() on the urban data frame to remove all rows that contain NA values. Store the cleaned data frame as urban_clean.
# import data from all three sheets in urbanpop.xls
path <- "urbanpop.xls"
urban_sheet1 <- read.xls(path, sheet = 1, stringsAsFactors = FALSE)
urban_sheet2 <- read.xls(path, sheet = 2, stringsAsFactors = FALSE)
urban_sheet3 <- read.xls(path, sheet = 3, stringsAsFactors = FALSE)

# Extend the cbind() call to include urban_sheet3: urban
urban <- cbind(urban_sheet1, urban_sheet2[-1], urban_sheet3[-1])

# Remove all rows with NAs from urban: 
urban_clean <- na.omit(urban)

# Print out a summary of urban_clean
summary(urban_clean)


## XLCONNECT Package
install.packages("XLConnect")
#interactively work with excel and R
#XLConnect works based on java, you might need to install Oracle's Java Development Kit (JDK) first
#XLConnect has functions in R for evey operation in excel

library("XLConnect")
#load a workbook as a workbook object
book <- loadWorkbook("cities.xlsx") 
getSheets(book) #gives us names of the sheets in our workbook

#we can read sheets at any point:
readWorksheet(book, sheet = 2, startRow = 1, endRow = 5, startCol = 2, header = FALSE)

#To get a clear overview about urbanpop.xlsx without having to open up the Excel file, you can execute the following code:
my_book <- loadWorkbook("urbanpop.xlsx")
sheets <- getSheets(my_book)
all <- lapply(sheets, readWorksheet, object = my_book)
str(all)

# Import columns 3, 4, and 5 from second sheet in my_book: urbanpop_sel
urbanpop_sel <- readWorksheet(my_book, sheet = 2, startCol = 3, endCol = 5)

# Import first column from second sheet in my_book: countries
countries <- readWorksheet(my_book, sheet = 2, startCol = 1, endCol = 1)

# cbind() urbanpop_sel and countries together: selection
selection <- cbind(countries, urbanpop_sel)


#adding data to a new sheet in a workbook
pop_2010 #data to be recorded
book <- loadWorkbook("cities.xlsx")
createSheet(book, name = "year_2010")
writeWorksheet(book, pop_2010, sheet = "year_2010")
saveWorkbook(book, file = "cities2.xlsx")

#we can rename sheets
renameSheet(book, "year_2010", "Y2010")

#or we can remove a sheet
removeSheet(book, sheet = "Y2010")


## use dplyr package to convert character rowns to numerics

#Example:
state_attend data keeps attandance numbers in all variables except 'states' variable
str(state_attend)
'data.frame':	52 obs. of  5 variables:
 $ state         : chr  "United States" "Alabama" "Alaska" "Arizona" ...
 $ avg_attend_pct: chr  "93.1" "93.8" "89.9" "89.0" ...
 $ avg_hr_per_day: chr  "6.6" "7.0" "6.5" "6.4" ...
 $ avg_day_per_yr: chr  "180" "180" "180" "181" ...
 $ avg_hr_per_yr : chr  "1193" "1267" "1163" "1159" ...

#convert all variables except 'states' into numbers
state_numeric <- mutate_at(state_attend, vars(-state), funs(as.numeric))
str(state_numeric)
'data.frame':	52 obs. of  5 variables:
 $ state         : chr  "United States" "Alabama" "Alaska" "Arizona" ...
 $ avg_attend_pct: num  93.1 93.8 89.9 89 91.8 93.2 93.9 87.9 89.8 91.2 ...
 $ avg_hr_per_day: num  6.6 7 6.5 6.4 6.9 6.2 7 6.5 6.7 6.9 ...
 $ avg_day_per_yr: num  180 180 180 181 179 181 171 181 181 181 ...
 $ avg_hr_per_yr : num  1193 1267 1163 1159 1229 ...


